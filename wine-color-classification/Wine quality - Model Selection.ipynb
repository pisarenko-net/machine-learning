{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Build ML pipelines and through cross-validation and assessment choose a model that performs the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print_function for compatibility with Python 3\n",
    "from __future__ import print_function\n",
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "# Scikit-Learn for Modeling\n",
    "import sklearn\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abt = pd.read_csv('abt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = abt.color\n",
    "X = abt.drop('color', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234, stratify=abt.color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5197, 11) (1300, 11)\n",
      "(5197,) (1300,)\n"
     ]
    }
   ],
   "source": [
    "## Check the size of train/test splits for red wine\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipelines and declare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(random_state=123, penalty='l1')),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(random_state=123, penalty='l2')),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=123)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n",
    "}\n",
    "\n",
    "# Hyperparameters to tune\n",
    "hyperparameters = {\n",
    "    'l1': {\n",
    "        'logisticregression__C': np.linspace(1e-3, 1e3, 10)\n",
    "    },\n",
    "    'l2': {\n",
    "        'logisticregression__C': np.linspace(1e-3, 1e3, 10)\n",
    "    },\n",
    "    'rf': {\n",
    "        'randomforestclassifier__n_estimators': [100, 200],\n",
    "        'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33]\n",
    "    },\n",
    "    'gb': {\n",
    "        'gradientboostingclassifier__n_estimators': [100, 200],\n",
    "        'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "        'gradientboostingclassifier__max_depth': [1,3,5]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cross-validation loops and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_models(X_train, y_train):\n",
    "    # Create empty dictionary called fitted_models\n",
    "    fitted_models = {}\n",
    "\n",
    "    # Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "    for pipeline in pipelines.keys():\n",
    "\n",
    "        # Create cross-validation object from pipeline and hyperparameters\n",
    "        model = GridSearchCV(pipelines[pipeline], hyperparameters[pipeline], cv=10, n_jobs=-1)\n",
    "\n",
    "        # Fit model on X_train, y_train\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store model in fitted_models[name] \n",
    "        fitted_models[pipeline] = model\n",
    "\n",
    "        # Print '{name} has been fitted'\n",
    "        print('{} has been fitted'.format(pipeline))\n",
    "        \n",
    "    return fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has been fitted\n",
      "l2 has been fitted\n",
      "rf has been fitted\n",
      "gb has been fitted\n"
     ]
    }
   ],
   "source": [
    "fitted_models = fit_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance and choose the winner\n",
    "\n",
    "Higher R^2 score and lower MAE is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.993842601501\n",
      "l2 0.993842601501\n",
      "rf 0.995381951126\n",
      "gb 0.995574369829\n"
     ]
    }
   ],
   "source": [
    "## Display best R^2 holdout score (red)\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_perf(fitted_models, X_test, y_test):\n",
    "    # Predict classes using L1-regularized logistic regression \n",
    "    for model in fitted_models.keys():\n",
    "        y_pred = fitted_models[model].predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(model, accuracy_score(y_test, y_pred))\n",
    "        print(cm)\n",
    "        print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.990769230769\n",
      "[[975   5]\n",
      " [  7 313]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       980\n",
      "          1       0.98      0.98      0.98       320\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1300\n",
      "\n",
      "l2 0.990769230769\n",
      "[[975   5]\n",
      " [  7 313]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       980\n",
      "          1       0.98      0.98      0.98       320\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1300\n",
      "\n",
      "rf 0.993846153846\n",
      "[[978   2]\n",
      " [  6 314]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       980\n",
      "          1       0.99      0.98      0.99       320\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1300\n",
      "\n",
      "gb 0.994615384615\n",
      "[[978   2]\n",
      " [  5 315]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       980\n",
      "          1       0.99      0.98      0.99       320\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluate performance of score prediction for reds\n",
    "evaluate_perf(fitted_models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.987085459184 0.993842601501\n",
      "l2 0.987091836735 0.993842601501\n",
      "rf 0.99912627551 0.995381951126\n",
      "gb 0.99856505102 0.995574369829\n"
     ]
    }
   ],
   "source": [
    "for model in fitted_models.keys():\n",
    "    pred = fitted_models[model].predict_proba(X_test)\n",
    "    pred = [p[1] for p in pred]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, pred)\n",
    "    print(model, auc(fpr, tpr), fitted_models[model].best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model based on random forest achieves 99% AUROC score. We can confidently predict the color of wine based on its physical characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
